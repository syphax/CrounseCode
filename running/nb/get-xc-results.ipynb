{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XC Result Collector\n",
    "\n",
    "This script collects events and event results from https://www.athletic.net/\n",
    "\n",
    "It is focused om Massachusetts High School XC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "* Improve logic for integrating new events into existing events\n",
    "* Improve logic so we only pull event details for new events (or ones we want to force updates for)\n",
    "* Pull single event results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import datetime\n",
    "import random\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "from typing import Dict, Type, Union\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Web scraping tools\n",
    "from bs4 import BeautifulSoup\n",
    "import asyncio\n",
    "from playwright.async_api import async_playwright\n",
    "from playwright.async_api import TimeoutError as PlaywrightTimeoutError\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = os.path.join('..', 'data')\n",
    "fn_events = 'db_events.csv'\n",
    "fn_event_details = 'db_event_details.csv'\n",
    "fn_athletes = 'db_athletes.csv'\n",
    "fn_results = 'db_results.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLAG_SAVE = True\n",
    "\n",
    "FLAG_UPDATE_EXISTING = False\n",
    "\n",
    "FLAG_DEBUG = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get pre-existing and new data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema\n",
    "\n",
    "dict_schema_events = {\n",
    "    'IDMeet': int,\n",
    "    'Sport': str,\n",
    "    'MeetName': str,\n",
    "    'SeasonId': int,\n",
    "    'StartDate': 'datetime64[ns]',\n",
    "    'EndDate': 'datetime64[ns]',\n",
    "    'Virtual': bool,\n",
    "    'LocationName': str,\n",
    "    'StreetAddress': str,  # nullable\n",
    "    'City': str,\n",
    "    'PostalCode': str,\n",
    "    'State': str,\n",
    "    'OwnerID': int,\n",
    "    'CalendarLock': int,\n",
    "    'UCalendarLock': int,\n",
    "    'RegEnd': 'datetime64[ns]',  # nullable\n",
    "    'HasResults': int,\n",
    "    'LevelMask': int,\n",
    "    'HostName': str,  # nullable\n",
    "    'MascotUrl': str,  # nullable\n",
    "    'Lat': float,\n",
    "    'Long': float,\n",
    "    'score': float,\n",
    "    'CalCount': int,\n",
    "    'OffDays': str,  # nullable, appears to be JSON string\n",
    "    'Data': str,  # nullable\n",
    "    'rsUrl': str,  # nullable\n",
    "    'LiveID': int,  # nullable\n",
    "    'LivePublished': bool,\n",
    "    'VideoURL': str,  # nullable\n",
    "    'Website': str,\n",
    "    'Country': str,\n",
    "    \n",
    "    # Additional columns:\n",
    "    \n",
    "    'dtRetrieved': 'datetime64[ns]',\n",
    "    'flagValidEvent': bool\n",
    "    #'flagXCEvent': bool\n",
    "    # 'flag_results_fetched': bool\n",
    "    \n",
    "}\n",
    "\n",
    "# Event Details\n",
    "dict_schema_event_details = {\n",
    "\n",
    "    'CourseName': str,         # String, can be None\n",
    "    'IDMeetDiv': int,          # Integer ID for division\n",
    "    'HyTekId': int,            # Integer ID from HyTek\n",
    "    'CourseId': int,           # Integer ID for course\n",
    "    'LevelMask': int,          # Integer representing competition level\n",
    "    'Gender': str,             # String ('M' or 'F')\n",
    "    'DivName': str,            # Full division name with distance\n",
    "    'Division': str,           # Short division name\n",
    "    'Meters': int,             # Race distance in meters\n",
    "    'Result': float,           # Numeric result, can be NaN\n",
    "    'RaceTime': str,           # ISO format datetime string, can be None\n",
    "    'Day': str,                # String representing day, can be None\n",
    "    'PlaceDepth': int,         # Integer for place depth\n",
    "    'ScoreDepth': int,         # Integer for score depth\n",
    "    'results': object,         # Can be None\n",
    "    'WarnScrollTo': str,       # String, can be None\n",
    "    'TeamScores': object,      # Can be None\n",
    "    'warnScrollTo': str,       # String, can be None\n",
    "    'warnSummaryString': str,  # String, can be None\n",
    "\n",
    "    # Additional columns:\n",
    "\n",
    "    'dtRetrieved': 'datetime64[ns]',  # Timestamp of data retrieval\n",
    "    'IDLocation': int,         # Location ID\n",
    "    'IDMeet': int              # Meet ID\n",
    "}\n",
    "\n",
    "# Results\n",
    "\n",
    "dict_schema_results = {\n",
    "    'Date': 'datetime64[ns]',\n",
    "    'IDMeetDiv': int, \n",
    "    'Gender': str, \n",
    "    'IDMeet': int, \n",
    "    'Location': int,\n",
    "    'Grade': int,\n",
    "    'AthleteID': int,\n",
    "    'First Name': str,\n",
    "    'Last Name': str,\n",
    "    'Name': str,\n",
    "    'Time': str,\n",
    "    'Time Dt': 'timedelta64[ns]',\n",
    "    'Seconds': float, \n",
    "    'Minutes': float,\n",
    "    'Team': str,\n",
    "    'TeamID': int,\n",
    "    'Team Count': int, \n",
    "    'Team Position': int, \n",
    "    'Place': int, \n",
    "    'Points': int\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def df_fix_types(df: pd.DataFrame, dict_schema: Dict[str, Union[Type, str]]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert DataFrame column types according to a specified schema dictionary.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        The input DataFrame whose column types need to be converted\n",
    "    dict_schema : Dict[str, Union[Type, str]]\n",
    "        Dictionary mapping column names to their desired types.\n",
    "        Supported types are:\n",
    "        - str: Converts to pandas string type\n",
    "        - int: Converts to nullable Int64 type\n",
    "        - float: Converts to float type\n",
    "        - bool: Converts to boolean type\n",
    "        - 'datetime64[ns]': Converts to datetime using ISO8601 format\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        A DataFrame with columns converted to specified types\n",
    "        \n",
    "    Raises\n",
    "    ------\n",
    "    Exception\n",
    "        If type conversion fails for any column, with details about the failure\n",
    "        \n",
    "    Examples\n",
    "    --------\n",
    "    >>> schema = {\n",
    "    ...     'id': int,\n",
    "    ...     'name': str,\n",
    "    ...     'value': float,\n",
    "    ...     'active': bool,\n",
    "    ...     'timestamp': 'datetime64[ns]'\n",
    "    ... }\n",
    "    >>> df_fixed = df_fix_types(df, schema)\n",
    "    \"\"\"\n",
    "    df = df.copy()  # Create a copy to avoid modifying the original DataFrame\n",
    "    \n",
    "    # Convert each column according to its type\n",
    "    for col, dtype in dict_schema.items():\n",
    "        if col in df.columns:\n",
    "            try:\n",
    "                if dtype == str:\n",
    "                    df[col] = df[col].astype('string')\n",
    "                elif dtype == int:\n",
    "                    df[col] = pd.to_numeric(df[col], errors='coerce').astype('Int64')\n",
    "                elif dtype == float:\n",
    "                    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "                elif dtype == bool:\n",
    "                    df[col] = df[col].astype(bool)\n",
    "                elif dtype == 'datetime64[ns]':\n",
    "                    df[col] = pd.to_datetime(df[col], format='ISO8601')\n",
    "            except Exception as e:\n",
    "                error_msg = f\"Error converting column {col} to {dtype}: {str(e)}\"\n",
    "                raise TypeError(error_msg) from e\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_df_events(response, schema):\n",
    "    \"\"\"\n",
    "    Create a DataFrame from the API response using a provided schema.\n",
    "    \n",
    "    Args:\n",
    "        response: requests.Response object from the API call\n",
    "        schema: dict mapping column names to their intended data types\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: Properly typed DataFrame containing the events data\n",
    "    \"\"\"\n",
    "    \n",
    "    # Parse JSON response\n",
    "    data = response.json()\n",
    "    \n",
    "    # Convert events list to DataFrame\n",
    "    df = pd.DataFrame(data['events'])\n",
    "    \n",
    "    # # Add retrieval timestamp for any datetime columns not in the response\n",
    "    # for col, dtype in schema.items():\n",
    "    #     if dtype == 'datetime64[ns]' and col not in df.columns:\n",
    "    #         df[col] = pd.Timestamp.now()\n",
    "            \n",
    "    # Hard-coded additional fields. Make sure the types are defined in the table schema1\n",
    "    \n",
    "    df['dtRetrieved'] = pd.Timestamp.now()\n",
    "    \n",
    "    df = df_fix_types(df, schema)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_event_details(meet_id, response, schema):\n",
    "    \n",
    "    data = response.json()\n",
    "    \n",
    "    if FLAG_DEBUG:\n",
    "        print('Meet ID: {}'.format(meet_id))\n",
    "        print(data)\n",
    "        for key in data.keys():\n",
    "            print(f\"{key}: {data[key]}\")\n",
    "    \n",
    "    if data is not None:\n",
    "        location_id = data['meet']['Location']['ID']\n",
    "        meet_id_response = data['meet']['ID']\n",
    "\n",
    "        df = pd.DataFrame(data['xcDivisions'])\n",
    "\n",
    "        df['IDLocation'] = location_id\n",
    "        df['dtRetrieved'] = pd.Timestamp.now()\n",
    "        df['IDMeet'] = meet_id_response\n",
    "\n",
    "        df = df_fix_types(df, schema)\n",
    "\n",
    "    else:\n",
    "        \n",
    "        print('No valid response for {}'.format(meet_id))\n",
    "        df = None\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_results(meet_div_id, response, schema):\n",
    "\n",
    "    data = response.json()\n",
    "    \n",
    "    if FLAG_DEBUG:\n",
    "        print('Meet Div ID: {}'.format(meet_div_id))\n",
    "        \n",
    "        print(data)\n",
    "        \n",
    "        for key in data.keys():\n",
    "            print(f\"{key}: {data[key]}\")\n",
    "    \n",
    "    if data is not None:\n",
    "        \n",
    "        df = None\n",
    "        \n",
    "        # print(data['currentEventValid'])\n",
    "        # print(data['resultsXC'])\n",
    "        \n",
    "    #     location_id = data['meet']['Location']['ID']\n",
    "    #     meet_id_response = data['meet']['ID']\n",
    "\n",
    "    #     df = pd.DataFrame(data['xcDivisions'])\n",
    "\n",
    "    #     df['IDLocation'] = location_id\n",
    "    #     df['dtRetrieved'] = pd.Timestamp.now()\n",
    "    #     df['IDMeet'] = meet_id_response\n",
    "\n",
    "    #     df = df_fix_types(df, schema)\n",
    "\n",
    "    else:\n",
    "        \n",
    "        print('No valid response for {}'.format(meet_div_id))\n",
    "        df = None\n",
    "\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_events(dt_start, dt_end, state_2, country_2='US', level=4):\n",
    "    \"\"\"\n",
    "    Retrieves athletic events data from athletic.net API for a specified state and date range.\n",
    "\n",
    "    Args:\n",
    "        state (str): Two-letter state code (e.g., 'MA' for Massachusetts)\n",
    "        dt_start (str): Start date in 'YYYY-MM-DD' format\n",
    "        dt_end (str): End date in 'YYYY-MM-DD' format\n",
    "        country_2 (str, optional): Two-letter country code. Defaults to 'US'\n",
    "        level (int, optional): Competition level filter. Defaults to 4\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame or None: DataFrame containing event information if request is successful,\n",
    "                                None if request fails. DataFrame structure is determined by \n",
    "                                create_df_events() function using dict_schema_events\n",
    "\n",
    "    Raises:\n",
    "        Potential requests.exceptions.RequestException: If the HTTP request fails\n",
    "\n",
    "    Example:\n",
    "        >>> events_df = get_events('MA', '2024-11-01', '2024-11-30')\n",
    "        >>> if events_df is not None:\n",
    "        ...     print(events_df.shape)\n",
    "\n",
    "    Notes:\n",
    "        - Makes a POST request to athletic.net's API endpoint\n",
    "        - Requires the create_df_events() function and dict_schema_events schema\n",
    "        - Some request headers are commented out but may be needed for authentication\n",
    "        - API endpoint: https://www.athletic.net/api/v1/Event/Events\n",
    "    \"\"\"\n",
    "\n",
    "    #state_name = 'Massachusetts'\n",
    "    \n",
    "    url = 'https://www.athletic.net/api/v1/Event/Events'\n",
    "\n",
    "    params = {\n",
    "        \"start\":dt_start,\n",
    "        \"end\":dt_end,\n",
    "        \"levelMask\":0,\n",
    "        \"sportMask\":0,\n",
    "        \"country\":country_2,\n",
    "        \"state\":state_2,\n",
    "        \"location\":\"\",\n",
    "        \"distanceKM\":0,\n",
    "        \"filterTerm\":\"\"}\n",
    "\n",
    "    # Make the POST request\n",
    "    headers = {\n",
    "        'content-type': 'application/json',\n",
    "        'authority': 'www.athletic.net',\n",
    "        'accept': 'application/json, text/plain, */*',\n",
    "        'accept-encoding': 'gzip, deflate, br, zstd',\n",
    "        'accept-language': 'en-US,en;q=0.9',\n",
    "        \n",
    "        # 'anet-appinfo': 'web:web:0:300',\n",
    "        # 'dnt': '1',\n",
    "        # 'origin': 'https://www.athletic.net',\n",
    "        # 'pageguid': 'c73bf291-0b5a-4062-a158-321e0c72c0f4',\n",
    "        # 'priority': 'u=1, i',\n",
    "        # 'referer': 'https://www.athletic.net/events/usa/massachusetts/2024-11-16;level=4',\n",
    "        # 'sec-ch-ua': '\"Chromium\";v=\"130\", \"Google Chrome\";v=\"130\", \"Not?A_Brand\";v=\"99\"',\n",
    "        # 'sec-ch-ua-mobile': '?0',\n",
    "        # 'sec-ch-ua-platform': '\"macOS\"',\n",
    "        # 'sec-fetch-dest': 'empty',\n",
    "        # 'sec-fetch-mode': 'cors',\n",
    "        # 'sec-fetch-site': 'same-origin',\n",
    "        # 'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, json=params, headers=headers)\n",
    "\n",
    "    # Check the response\n",
    "    if response.status_code == 200:\n",
    "        print(\"Request successful!\")\n",
    "        \n",
    "        # TODO: Make the schema a parameter\n",
    "        df_events_new = create_df_events(response, dict_schema_events)\n",
    "        \n",
    "    else:\n",
    "        print(f\"Request failed with status code: {response.status_code}\")\n",
    "        print(f\"Response: {response.text}\")\n",
    "        \n",
    "        df_events_new = None\n",
    "        \n",
    "    return df_events_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_event_details(meet_id):\n",
    "\n",
    "    url = 'https://www.athletic.net/api/v1/Meet/GetMeetData?meetId={}&sport=xc'.format(meet_id)\n",
    "\n",
    "    headers = {\n",
    "        'content-type': 'application/json',\n",
    "        'authority': 'www.athletic.net',\n",
    "        'accept': 'application/json, text/plain, */*',\n",
    "        'accept-encoding': 'gzip, deflate, br, zstd',\n",
    "        'accept-language': 'en-US,en;q=0.9'\n",
    "    }\n",
    "\n",
    "    # Make the GET request\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    # Check if request was successful\n",
    "    if response.status_code == 200:\n",
    "        print(\"Request successful!\")\n",
    "        \n",
    "        df_event_details_new = create_df_event_details(meet_id, response, dict_schema_event_details)\n",
    "        \n",
    "    else:\n",
    "        print(f\"Request failed with status code: {response.status_code}\")\n",
    "        print(f\"Response: {response.text}\")\n",
    "        \n",
    "        df_event_details_new = None\n",
    "        \n",
    "    return df_event_details_new\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to start a persistent browser\n",
    "async def start_browser():\n",
    "    \"\"\"\n",
    "    Starts a persistent Playwright browser instance.\n",
    "\n",
    "    Returns:\n",
    "        Browser: A Playwright browser instance.\n",
    "    \"\"\"\n",
    "    playwright = await async_playwright().start()\n",
    "    browser = await playwright.chromium.launch(headless=True)\n",
    "    return browser, playwright\n",
    "\n",
    "# Fetch a page using the persistent browser\n",
    "# async def fetch_page(browser, url):\n",
    "#     \"\"\"\n",
    "#     Fetches a webpage using an existing Playwright browser instance and returns a BeautifulSoup object.\n",
    "\n",
    "#     Parameters:\n",
    "#         browser (Browser): A Playwright browser instance.\n",
    "#         url (str): The URL of the webpage to fetch.\n",
    "\n",
    "#     Returns:\n",
    "#         BeautifulSoup: Parsed BeautifulSoup object containing the rendered HTML.\n",
    "#     \"\"\"\n",
    "#     page = await browser.new_page()\n",
    "#     await page.goto(url)\n",
    "#     await page.wait_for_load_state(\"networkidle\")\n",
    "#     rendered_html = await page.content()\n",
    "#     await page.close()\n",
    "#     return BeautifulSoup(rendered_html, 'html.parser')\n",
    "\n",
    "async def fetch_page(browser, url, max_retries=3, timeout=60000):\n",
    "    retries = 0\n",
    "    page = None\n",
    "    \n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            if page is not None:\n",
    "                await page.close()\n",
    "            \n",
    "            page = await browser.new_page()\n",
    "            page.set_default_timeout(timeout)  # Removed await - this is synchronous\n",
    "            \n",
    "            # Navigate to the page with a more lenient load condition\n",
    "            response = await page.goto(url, wait_until=\"domcontentloaded\")\n",
    "            if not response or not response.ok:\n",
    "                raise Exception(f\"Failed to load page: {response.status if response else 'No response'}\")\n",
    "            \n",
    "            try:\n",
    "                # Try to wait for network idle, but don't fail if it times out\n",
    "                await page.wait_for_load_state(\"networkidle\", timeout=30000)\n",
    "            except PlaywrightTimeoutError:\n",
    "                print(f\"Network idle timeout on attempt {retries + 1}, proceeding with partial load\")\n",
    "            \n",
    "            rendered_html = await page.content()\n",
    "            await page.close()\n",
    "            return BeautifulSoup(rendered_html, 'html.parser') \n",
    "            \n",
    "        except Exception as e:\n",
    "            retries += 1\n",
    "            if page:\n",
    "                try:\n",
    "                    await page.close()\n",
    "                except:\n",
    "                    pass\n",
    "                    \n",
    "            if retries == max_retries:\n",
    "                raise Exception(f\"Failed to fetch page after {max_retries} attempts: {str(e)}\")\n",
    "                \n",
    "            print(f\"Attempt {retries} failed: {str(e)}, retrying...\")\n",
    "            await asyncio.sleep(2 * retries)\n",
    "            \n",
    "            \n",
    "\n",
    "# Function to close the persistent browser\n",
    "async def close_browser(browser, playwright):\n",
    "    \"\"\"\n",
    "    Closes the persistent Playwright browser instance.\n",
    "\n",
    "    Parameters:\n",
    "        browser (Browser): A Playwright browser instance.\n",
    "        playwright: The Playwright object instance.\n",
    "    \"\"\"\n",
    "    await browser.close()\n",
    "    await playwright.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results_from_html_athletic(soup):\n",
    "    \"\"\"\n",
    "    Parses the HTML content (soup) and extracts race results into a structured DataFrame.\n",
    "    This function works specifically with results from athletic.net\n",
    "\n",
    "    Parameters:\n",
    "        soup (BeautifulSoup): The BeautifulSoup object of the rendered HTML.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing the extracted results.\n",
    "    \"\"\"\n",
    "\n",
    "    # Find all result rows\n",
    "    result_rows = soup.find_all(\"div\", class_=\"result-row\")\n",
    "\n",
    "    # Initialize a list to store data\n",
    "    data = []\n",
    "\n",
    "    # Iterate through each result row\n",
    "    for row in result_rows:\n",
    "        # Extract place\n",
    "        place = row.find(\"div\", class_=\"place-column\").text.strip()\n",
    "\n",
    "        # Extract full name\n",
    "        # name_tag = row.find(\"a\", ashrefonweb=\"\")\n",
    "        # name = name_tag.text.strip() if name_tag else None\n",
    "\n",
    "        # name_tag = row.find(\"a\", ashrefonweb=True, class_=\"ng-star-inserted\")\n",
    "        # name = name_tag.text.strip() if name_tag else None\n",
    "            \n",
    "        # Extract full name and athlete ID\n",
    "        name_tag = row.find(\"a\", ashrefonweb=True, class_=\"ng-star-inserted\")\n",
    "        name = name_tag.text.strip() if name_tag else None\n",
    "        athlete_id = None\n",
    "        if name_tag and \"href\" in name_tag.attrs:\n",
    "            href = name_tag[\"href\"]\n",
    "            athlete_id = href.split(\"/\")[2]  # Extract the ID from the URL\n",
    "            \n",
    "        # # Extract team (from second <a> in \"subtitle team\" section)\n",
    "        # team_tag = row.find(\"div\", class_=\"subtitle team\")\n",
    "        # team = None\n",
    "        # if team_tag:\n",
    "        #     team_a_tag = team_tag.find_all(\"a\", ashrefonweb=True)\n",
    "        #     if len(team_a_tag) > 1:\n",
    "        #         team = team_a_tag[1].text.strip()\n",
    "        \n",
    "        # Extract team name and team ID\n",
    "        team_tag = row.find(\"div\", class_=\"subtitle team\")\n",
    "        team = None\n",
    "        team_id = None\n",
    "        if team_tag:\n",
    "            team_a_tag = team_tag.find_all(\"a\", ashrefonweb=True)\n",
    "            if len(team_a_tag) > 1:\n",
    "                team = team_a_tag[1].text.strip()\n",
    "                if \"href\" in team_a_tag[1].attrs:\n",
    "                    team_href = team_a_tag[1][\"href\"]\n",
    "                    team_id = team_href.split(\"/\")[2]  # Extract the ID from \"/team/{id}/cross-country\"\n",
    "\n",
    "        \n",
    "        # Extract time\n",
    "        time_tag = row.find(\"div\", class_=\"secondary\").find(\"a\", class_=\"ng-star-inserted\")\n",
    "        time = time_tag.text.strip() if time_tag else None\n",
    "\n",
    "        # Extract year (at most 2 characters)\n",
    "        year_tag = row.find(\"shared-tertiary-stats\").find(\"span\", string=lambda t: t and \"Yr:\" in t)\n",
    "        year = year_tag.text.replace(\"Yr:\", \"\").strip()[:2] if year_tag else None\n",
    "\n",
    "        # Extract points (strip \"+\" and unnecessary characters)\n",
    "        points_tag = row.find(\"shared-tertiary-stats\").find(\"span\", string=lambda t: t and \"pts\" in t)\n",
    "        points = (\n",
    "            points_tag.text.split(\"•\")[-1].replace(\"pts\", \"\").replace(\"+\", \"\").strip()\n",
    "            if points_tag\n",
    "            else None\n",
    "        )\n",
    "\n",
    "        # Add to data list\n",
    "        data.append({\n",
    "            \"Place\": place,\n",
    "            \"Name\": name,\n",
    "            \"Athlete ID\": athlete_id,\n",
    "            \"Team\": team,\n",
    "            \"Team ID\": team_id,\n",
    "            \"Time\": time,\n",
    "            \"Year\": year,\n",
    "            \"Points\": points,\n",
    "        })\n",
    "\n",
    "    # Convert to a pandas DataFrame for better presentation\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backup_if_exists(filepath):\n",
    "    \"\"\"\n",
    "    Create a timestamped backup of a file if it exists.\n",
    "    \n",
    "    Args:\n",
    "        filepath (str): Path to the file to backup\n",
    "        ignore_files (list, optional): List of filenames to ignore in the target directory\n",
    "    \"\"\"\n",
    "    \n",
    "    if not os.path.exists(filepath):\n",
    "        return\n",
    "        \n",
    "    # Split the path into directory, filename, and extension\n",
    "    directory = os.path.dirname(filepath)\n",
    "    filename = os.path.basename(filepath)\n",
    "    \n",
    "    name, ext = os.path.splitext(filename)\n",
    "    \n",
    "    # Get current timestamp for backup file\n",
    "    timestamp = datetime.now().strftime('%H-%M-%S')\n",
    "    print('Time: {}'.format(timestamp))\n",
    "    \n",
    "    # Create backup filename with timestamp\n",
    "    backup_filename = f\"{name}-BAK-{timestamp}{ext}\"\n",
    "    backup_path = os.path.join(directory, backup_filename)\n",
    "    \n",
    "    # Create the backup\n",
    "    shutil.copy2(filepath, backup_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEV AREA FOR RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TEST FOR EXTRACTING RESULTS FOR A SINGLE PAGE\n",
    "\n",
    "# Set to True to run this code\n",
    "\n",
    "if False:\n",
    "    \n",
    "    async def fetch_page():\n",
    "        async with async_playwright() as p:\n",
    "            # Launch browser in headless mode\n",
    "            browser = await p.chromium.launch(headless=True)\n",
    "            page = await browser.new_page()\n",
    "\n",
    "            # Navigate to the page\n",
    "            #await page.goto(\"https://www.athletic.net/CrossCountry/meet/250591/results/1001120\")\n",
    "            \n",
    "            await page.goto(\"https://www.athletic.net/CrossCountry/meet/250562/results/997016\")\n",
    "\n",
    "            # Wait for network activity to finish\n",
    "            await page.wait_for_load_state(\"networkidle\")\n",
    "\n",
    "            # Get the rendered HTML\n",
    "            rendered_html = await page.content()\n",
    "\n",
    "            # Parse with BeautifulSoup\n",
    "            soup = BeautifulSoup(rendered_html, 'html.parser')\n",
    "            print(soup.prettify())\n",
    "            with open(os.path.join(path_data, 'test_results.html'), 'w', encoding='utf-8') as file:\n",
    "                file.write(soup.prettify())\n",
    "            \n",
    "            # Close the browser\n",
    "            await browser.close()\n",
    "\n",
    "    # Use 'await' directly in Jupyter Notebook\n",
    "    await fetch_page()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "\n",
    "    # Start the browser\n",
    "    browser, playwright = await start_browser()\n",
    "\n",
    "    try:\n",
    "        # Fetch multiple pages\n",
    "        urls = [\n",
    "            \"https://www.athletic.net/CrossCountry/meet/250562/results/997016\",\n",
    "            #\"https://www.athletic.net/CrossCountry/meet/250591/results/1001120\"\n",
    "        ]\n",
    "        for url in urls:\n",
    "            soup = await fetch_page(browser, url)\n",
    "            print(soup.title.string)  # Example: Print the page title\n",
    "    finally:\n",
    "        # Close the browser\n",
    "        print('OK!')\n",
    "        \n",
    "    #print(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Events\n",
    "\n",
    "pe = os.path.join(path_data, fn_events)\n",
    "\n",
    "if os.path.isfile(pe):\n",
    "    df_events = pd.read_csv(pe)\n",
    "    df_events = df_fix_types(df_events, dict_schema_events)\n",
    "    print('Loaded df_events: {} rows'.format(len(df_events)))\n",
    "else:\n",
    "    df_events = pd.DataFrame({col: pd.Series(dtype=dtype) for col, dtype in dict_schema_events.items()})\n",
    "    print('Initializing blank df_events')\n",
    "\n",
    "\n",
    "# Event Details\n",
    "\n",
    "ped = os.path.join(path_data, fn_event_details)\n",
    "\n",
    "if os.path.isfile(ped):\n",
    "    df_event_details = pd.read_csv(ped)\n",
    "    df_event_details = df_fix_types(df_event_details, dict_schema_event_details)\n",
    "    print('Loaded df_event_details: {} rows'.format(len(df_event_details)))\n",
    "else:\n",
    "    df_event_details = pd.DataFrame({col: pd.Series(dtype=dtype) for col, dtype in dict_schema_event_details.items()})\n",
    "    print('Initializing blank df_event_details')\n",
    "\n",
    "\n",
    "# Results\n",
    "\n",
    "pr = os.path.join(path_data, fn_results)\n",
    "\n",
    "if os.path.isfile(pr):\n",
    "    df_results = pd.read_csv(pr)\n",
    "    print('Loaded df_results: {} rows'.format(len(df_results)))\n",
    "\n",
    "else:\n",
    "    df_results = pd.DataFrame({col: pd.Series(dtype=dtype) for col, dtype in dict_schema_results.items()})\n",
    "    print('Initializing blank df_results')\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get new events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hard-coded dates\n",
    "# Use this to back-fill older events\n",
    "\n",
    "df_events_new = get_events('2024-11-01', '2024-12-31', 'MA')\n",
    "set_new_events = set(df_events_new['IDMeet'])\n",
    "\n",
    "print(len(df_events_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to df_events, remove duplicates\n",
    "\n",
    "# TODO: Add logic to make updating more flexible\n",
    "\n",
    "set_existing_events = set(df_events['IDMeet'])\n",
    "\n",
    "cnt_events_existing = len(df_events)\n",
    "cnt_events_new = len(df_events_new)\n",
    "\n",
    "df_events = pd.concat([df_events, df_events_new], axis=0)\n",
    "\n",
    "df_events = df_events.dropna(how='all')\n",
    "\n",
    "df_events = df_events.sort_values('dtRetrieved', ascending=True).drop_duplicates(subset=['IDMeet'], keep='last')\n",
    "\n",
    "df_events = df_fix_types(df_events, dict_schema_events)\n",
    "\n",
    "cnt_events_total = len(df_events)\n",
    "\n",
    "cnt_dupes = cnt_events_existing + cnt_events_new - cnt_events_total \n",
    "\n",
    "print('Merged {} events into existing set of {}; final count is {}. {} duplicates'.format(cnt_events_new, cnt_events_existing, cnt_events_total, cnt_dupes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Currently, this block is controlled with comment/uncomments\n",
    "\n",
    "# Get event details for all newly retrieved events\n",
    "\n",
    "df_get_these_event_details = df_events_new\n",
    "\n",
    "# Get event details for a sample of N events in df_events that are not in df_event_details\n",
    "\n",
    "# max_events = 150\n",
    "\n",
    "# set_events = set(df_events.loc[df_events['Sport']=='XC', 'IDMeet'])\n",
    "\n",
    "# set_existing_event_details = set(df_event_details['IDMeet'])\n",
    "\n",
    "# set_events_to_get = set_events - set_existing_event_details\n",
    "\n",
    "# df_get_these_event_details = df_events[df_events['IDMeet'].isin(set_events_to_get)]\n",
    "\n",
    "# if len(df_get_these_event_details) < max_events:\n",
    "#     max_events = len(df_get_these_event_details)\n",
    "    \n",
    "# df_get_these_event_details = df_get_these_event_details.sample(max_events)\n",
    "\n",
    "# print(len(set_events), len(set_existing_event_details), len(set_events_to_get), len(df_get_these_event_details))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-time to initialize flagValidEvent\n",
    "\n",
    "# df_events['flagValidEvent'] = True\n",
    "# df_events.loc[df_events['IDMeet'].isin(set_events_to_get), 'flagValidEvent'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get new event details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get new event details\n",
    "\n",
    "list_df_event_details = []\n",
    "list_inactive_events = []\n",
    "\n",
    "for i, r in df_get_these_event_details.iterrows():\n",
    "    \n",
    "    meet_id = r['IDMeet']\n",
    "    \n",
    "    print(i, meet_id)\n",
    "    \n",
    "    if r['Sport'] == 'XC':\n",
    "        _df = get_event_details(meet_id)\n",
    "        \n",
    "        if _df is None:\n",
    "            print('Failed to get details for {}'.format(r['MeetName']))\n",
    "            list_inactive_events.append(meet_id)\n",
    "        else:\n",
    "            if len(_df) == 0:\n",
    "                print('No event details for {}'.format(r['MeetName']))\n",
    "                list_inactive_events.append(meet_id)\n",
    "            else:\n",
    "                print('Got {} event details'.format(len(_df)))\n",
    "                list_df_event_details.append(_df)\n",
    "                \n",
    "    else:\n",
    "        print('Not an XC event')\n",
    "        list_inactive_events.append(meet_id)\n",
    "\n",
    "if len(list_df_event_details) > 0:\n",
    "    df_new_event_details = pd.concat(list_df_event_details, axis=0)\n",
    "else:\n",
    "    print('No new valid events found')\n",
    "\n",
    "# Tag any past event for which there was no info as invalid\n",
    "dt_now = datetime.datetime.now()\n",
    "df_events.loc[df_events['IDMeet'].isin(list_inactive_events) & (df_events['EndDate'] < dt_now), 'flagValidEvent'] = False\n",
    "\n",
    "print(len(df_new_event_details))\n",
    "df_new_event_details = df_new_event_details.drop_duplicates()\n",
    "print(len(df_new_event_details))\n",
    "\n",
    "# TODO: Fix errors here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_event_details))\n",
    "\n",
    "if len(df_new_event_details) > 0:\n",
    "    df_event_details = pd.concat([df_event_details, df_new_event_details], axis=0)\n",
    "else:\n",
    "    print('No new event details')\n",
    "print(len(df_event_details))\n",
    "\n",
    "df_event_details = df_event_details.drop_duplicates()\n",
    "\n",
    "print(len(df_event_details))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Checks\n",
    "\n",
    "set_events = set(df_events.loc[df_events['Sport']=='XC', 'IDMeet'])\n",
    "\n",
    "set_events_valid = set(df_events.loc[(df_events['Sport']=='XC') & (df_events['flagValidEvent']==True), 'IDMeet'])\n",
    "\n",
    "set_events_in_event_details = set(df_event_details['IDMeet'])\n",
    "\n",
    "events_in_e_not_ed = set_events - set_events_in_event_details\n",
    "events_valid_in_e_not_ed = set_events_valid - set_events_in_event_details\n",
    "\n",
    "events_in_ed_not_e = set_events_in_event_details - set_events\n",
    "\n",
    "print('There are {} events without event details'.format(len(events_in_e_not_ed)))\n",
    "\n",
    "# This should be zero\n",
    "print('There are {} valid events without event details'.format(len(events_valid_in_e_not_ed)))\n",
    "\n",
    "# This should be zero\n",
    "print('There are {} events with event details that are not in the events table'.format(len(events_in_ed_not_e)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get new results\n",
    "\n",
    "* Start browser\n",
    "* Get a list of event details with no results\n",
    "* Get some results\n",
    "* Save\n",
    "\n",
    "2 modes:\n",
    "* Build historical results\n",
    "* Get recent results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get new results\n",
    "\n",
    "# Figure out what events to get:\n",
    "\n",
    "# At event level\n",
    "\n",
    "# set_existing_results_div = set(df_results['IDMeetDiv'])\n",
    "# set_results_div_to_get = set(df_event_details['IDMeetDiv']) - set_existing_results_div\n",
    "\n",
    "# At meet level\n",
    "\n",
    "set_existing_meets_with_results = set(df_results['IDMeet'])\n",
    "set_meet_results_to_get = set(df_event_details['IDMeet']) - set_existing_meets_with_results\n",
    "\n",
    "#set_results_div_to_get = set(df_event_details.loc[df_event_details['IDMeet'].isin(set_meet_results_to_get), 'IDMeetDiv'])\n",
    "\n",
    "# Currently we just randomly grab a few:\n",
    "\n",
    "#TODO: Change this to grab results for N meets, not N events\n",
    "\n",
    "max_results = 5\n",
    "\n",
    "sample_set = set(random.sample(list(set_meet_results_to_get), max_results))\n",
    "\n",
    "df_event_details_for_new_results = df_event_details[df_event_details['IDMeet'].isin(sample_set)]\n",
    "\n",
    "cnt_new_event_details = len(df_event_details_for_new_results)\n",
    "\n",
    "print('Preparing to fetch results for {} events in {} meets'.format(cnt_new_event_details, max_results))\n",
    "print(list(sample_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warm up the browser...\n",
    "browser, playwright = await start_browser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get the results!\n",
    "\n",
    "list_df_event_results = []\n",
    "list_bad_results = []\n",
    "\n",
    "for i, r in df_event_details_for_new_results.iterrows():\n",
    "    \n",
    "    meet_div_id = r['IDMeetDiv']\n",
    "    meet_id = r['IDMeet']\n",
    "    meet_name = df_events.loc[df_events['IDMeet'] == meet_id, 'MeetName']\n",
    "    \n",
    "    if FLAG_DEBUG:\n",
    "        print(i, meet_id, meet_div_id)\n",
    "    \n",
    "    url = 'https://www.athletic.net/CrossCountry/meet/{}/results/{}'.format(meet_id, meet_div_id)\n",
    "    \n",
    "    #if FLAG_DEBUG:\n",
    "    print(i, url)\n",
    "    \n",
    "    results_soup = await fetch_page(browser, url)\n",
    "    \n",
    "    if FLAG_DEBUG:\n",
    "        print(type(results_soup))\n",
    "        \n",
    "    _df = get_results_from_html_athletic(results_soup)\n",
    "    \n",
    "    if _df is None:\n",
    "        print('Failed to get results for {} at {}'.format(meet_div_id, meet_name))\n",
    "        list_bad_results.append(meet_div_id)\n",
    "    else:\n",
    "        if len(_df) == 0:\n",
    "            print('No results for {} at {}'.format(meet_div_id, meet_name))\n",
    "            list_bad_results.append(meet_div_id)\n",
    "        else:\n",
    "            print('Got {} results'.format(len(_df)))\n",
    "            _df['IDMeet'] = meet_id\n",
    "            _df['IDMeetDiv'] = meet_div_id\n",
    "            list_df_event_results.append(_df)\n",
    "                \n",
    "    # else:\n",
    "    #     print('Not an XC event')\n",
    "    #     list_bad_results.append(meet_id)\n",
    "\n",
    "if len(list_df_event_results) > 0:\n",
    "    df_new_event_results = pd.concat(list_df_event_results, axis=0)\n",
    "else:\n",
    "    print('No new valid results found')\n",
    "\n",
    "\n",
    "print('DONE')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the browser\n",
    "\n",
    "close_browser(browser, playwright)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate to results\n",
    "\n",
    "if len(df_new_event_results) > 0:\n",
    "\n",
    "    df_results = pd.concat([df_results, df_new_event_results], axis=0)\n",
    "    \n",
    "    df_results = df_results.drop_duplicates()\n",
    "    \n",
    "print(df_results.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cnt_meets_all = df_events['IDMeet'].nunique()\n",
    "cnt_events_all = df_event_details['IDMeetDiv'].nunique()\n",
    "\n",
    "cnt_meets_results = df_results['IDMeet'].nunique()\n",
    "cnt_events_results = df_results['IDMeetDiv'].nunique()\n",
    "\n",
    "print('There are {} meets covering {} events in the meet/event data'.format(cnt_meets_all, cnt_events_all))\n",
    "print('There are results for {} meets covering {} events in the results data'.format(cnt_meets_results, cnt_events_results))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data!\n",
    "\n",
    "if FLAG_SAVE:\n",
    "\n",
    "    # Create backups if files exist\n",
    "    print('Backing files up...')\n",
    "    backup_if_exists(pe)\n",
    "    backup_if_exists(ped)\n",
    "    backup_if_exists(pr)\n",
    "    \n",
    "    # Save the new files\n",
    "    print('Saving files...')\n",
    "    df_events.to_csv(pe, index=False)\n",
    "    df_event_details.to_csv(ped, index=False)\n",
    "    df_results.to_csv(pr, index=False)\n",
    "    \n",
    "    print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ".columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ad-Hoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.athletic.net/api/v1/public/GetStatesCountries2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all events at Wrentham\n",
    "\n",
    "#df_events[df_events['LocationName'].str.contains('wrentham', case=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
